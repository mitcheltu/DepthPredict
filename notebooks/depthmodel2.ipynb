{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b3ba79",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-04T08:06:28.002367Z",
     "iopub.status.busy": "2025-09-04T08:06:28.002073Z",
     "iopub.status.idle": "2025-09-04T09:20:55.464530Z",
     "shell.execute_reply": "2025-09-04T09:20:55.463154Z"
    },
    "papermill": {
     "duration": 4467.503787,
     "end_time": "2025-09-04T09:20:55.501894",
     "exception": false,
     "start_time": "2025-09-04T08:06:27.998107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b48c78306b4acda7d48f50d3574439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:21<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4380 | Val Loss: 1.4995 | Val RMSE: 1.2245 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 2/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:17<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3853 | Val Loss: 1.3745 | Val RMSE: 1.1724 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 3/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:18<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2904 | Val Loss: 1.1578 | Val RMSE: 1.0760 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 4/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:17<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0122 | Val Loss: 1.1405 | Val RMSE: 1.0680 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 5/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:17<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0188 | Val Loss: 1.1783 | Val RMSE: 1.0855 m\n",
      "Training complete\n",
      "\n",
      "Epoch 1/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:34<00:00,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8768 | Val Loss: 0.9554 | Val RMSE: 0.9774 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 2/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:43<00:00,  8.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7352 | Val Loss: 0.8694 | Val RMSE: 0.9324 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 3/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:33<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6385 | Val Loss: 0.8446 | Val RMSE: 0.9190 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 4/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:25<00:00,  7.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6054 | Val Loss: 0.8500 | Val RMSE: 0.9219 m\n",
      "\n",
      "Epoch 5/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:34<00:00,  8.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5508 | Val Loss: 0.8362 | Val RMSE: 0.9144 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 6/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:29<00:00,  7.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5450 | Val Loss: 0.7529 | Val RMSE: 0.8677 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 7/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:27<00:00,  7.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4894 | Val Loss: 0.7629 | Val RMSE: 0.8734 m\n",
      "\n",
      "Epoch 8/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:36<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4867 | Val Loss: 0.7772 | Val RMSE: 0.8816 m\n",
      "\n",
      "Epoch 9/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:29<00:00,  7.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4498 | Val Loss: 0.6770 | Val RMSE: 0.8228 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 10/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:27<00:00,  7.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4432 | Val Loss: 0.7468 | Val RMSE: 0.8642 m\n",
      "\n",
      "Epoch 11/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:29<00:00,  7.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4280 | Val Loss: 0.6970 | Val RMSE: 0.8349 m\n",
      "\n",
      "Epoch 12/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:29<00:00,  7.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3982 | Val Loss: 0.7267 | Val RMSE: 0.8525 m\n",
      "\n",
      "Epoch 13/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:38<00:00,  8.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3864 | Val Loss: 0.6881 | Val RMSE: 0.8295 m\n",
      "\n",
      "Epoch 14/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:28<00:00,  7.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3794 | Val Loss: 0.7001 | Val RMSE: 0.8367 m\n",
      "\n",
      "Epoch 15/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:30<00:00,  7.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3635 | Val Loss: 0.6803 | Val RMSE: 0.8248 m\n",
      "\n",
      "Epoch 16/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:30<00:00,  7.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3668 | Val Loss: 0.6995 | Val RMSE: 0.8364 m\n",
      "\n",
      "Epoch 17/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:34<00:00,  8.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3735 | Val Loss: 0.6753 | Val RMSE: 0.8218 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 18/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:32<00:00,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3655 | Val Loss: 0.7089 | Val RMSE: 0.8420 m\n",
      "\n",
      "Epoch 19/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:44<00:00,  8.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3627 | Val Loss: 0.6773 | Val RMSE: 0.8230 m\n",
      "\n",
      "Epoch 20/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:32<00:00,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3538 | Val Loss: 0.6586 | Val RMSE: 0.8115 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 21/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:32<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3606 | Val Loss: 0.6617 | Val RMSE: 0.8134 m\n",
      "\n",
      "Epoch 22/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:29<00:00,  7.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3549 | Val Loss: 0.6337 | Val RMSE: 0.7961 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 23/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:30<00:00,  7.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3525 | Val Loss: 0.6338 | Val RMSE: 0.7961 m\n",
      "\n",
      "Epoch 24/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:33<00:00,  8.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3515 | Val Loss: 0.6715 | Val RMSE: 0.8195 m\n",
      "\n",
      "Epoch 25/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:31<00:00,  7.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3506 | Val Loss: 0.6248 | Val RMSE: 0.7905 m\n",
      "✅ Best model saved.\n",
      "Training complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6248449270541852"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as F_nn\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import timm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def paired_transform(color_img, depth_img, output_size=224, max_depth=10.0, apply_color_jitter=True):\n",
    "    if random.random() > 0.5:\n",
    "        color_img = F.hflip(color_img)\n",
    "        depth_img = F.hflip(depth_img)\n",
    "\n",
    "    angle = random.uniform(-15, 15)\n",
    "    color_img = F.rotate(color_img, angle, interpolation=Image.BILINEAR)\n",
    "    depth_img = F.rotate(depth_img, angle, interpolation=Image.NEAREST)\n",
    "\n",
    "    i, j, h, w = transforms.RandomResizedCrop.get_params(\n",
    "        color_img, scale=(0.8, 1.0), ratio=(1.0, 1.0)\n",
    "    )\n",
    "    color_img = F.resized_crop(color_img, i, j, h, w, size=(output_size, output_size))\n",
    "    depth_img = F.resized_crop(depth_img, i, j, h, w, size=(output_size, output_size))\n",
    "\n",
    "    if apply_color_jitter:\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3)\n",
    "        color_img = color_jitter(color_img)\n",
    "\n",
    "    # RGB → tensor\n",
    "    color_tensor = F.to_tensor(color_img)\n",
    "    color_tensor = F.normalize(color_tensor, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "\n",
    "    # Depth → tensor (meters)\n",
    "    depth_np = np.array(depth_img).astype(np.float32) / 1000.0\n",
    "    depth_tensor = torch.from_numpy(depth_np).unsqueeze(0)\n",
    "    depth_tensor = torch.clamp(depth_tensor, 0, max_depth)\n",
    "\n",
    "    return color_tensor, depth_tensor\n",
    "    \n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, data_dir, paired_transform=None, max_depth=10.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to folder containing 'colors/' and 'depths/' subfolders.\n",
    "            paired_transform (callable, optional): Function to apply same geometric transform to both RGB and depth.\n",
    "            max_depth (float): Maximum depth value to scale depth maps (in meters).\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.paired_transform = paired_transform\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Paths to color and depth folders\n",
    "        self.color_dir = os.path.join(data_dir, \"colors\")\n",
    "        self.depth_dir = os.path.join(data_dir, \"depths\")\n",
    "\n",
    "        # List all RGB color images\n",
    "        self.color_files = sorted([f for f in os.listdir(self.color_dir) if f.endswith(\"_colors.png\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.color_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load RGB image\n",
    "        color_path = os.path.join(self.color_dir, self.color_files[idx])\n",
    "        color_img = Image.open(color_path).convert(\"RGB\")\n",
    "\n",
    "        # Load corresponding depth image\n",
    "        depth_file = self.color_files[idx].replace(\"_colors.png\", \"_depth.png\")\n",
    "        depth_path = os.path.join(self.depth_dir, depth_file)\n",
    "        depth_img = Image.open(depth_path)\n",
    "\n",
    "        # Apply paired transform if provided\n",
    "        if self.paired_transform:\n",
    "            color_tensor, depth_tensor = self.paired_transform(color_img, depth_img)\n",
    "        else:\n",
    "            # Convert RGB to tensor and normalize\n",
    "            color_tensor = T.ToTensor()(color_img)\n",
    "            color_tensor = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                       std=[0.229, 0.224, 0.225])(color_tensor)\n",
    "            # Convert depth to tensor and scale to meters\n",
    "            depth_np = np.array(depth_img).astype(np.float32) / 1000.0  # mm → meters\n",
    "            depth_tensor = torch.from_numpy(depth_np).unsqueeze(0)\n",
    "            depth_tensor = torch.clamp(depth_tensor, 0, self.max_depth)\n",
    "\n",
    "        return color_tensor, depth_tensor\n",
    "\n",
    "\n",
    "class DepthModel(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_b3', pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --------------------------\n",
    "        # Encoder: pretrained CNN\n",
    "        # --------------------------\n",
    "        # Use features_only=True to get intermediate feature maps for decoder\n",
    "        self.encoder = timm.create_model(backbone_name, pretrained=pretrained, features_only=True)\n",
    "        \n",
    "        # Channels of encoder feature maps at each stage\n",
    "        encoder_channels = self.encoder.feature_info.channels()  # e.g., [40, 48, 136, 384]\n",
    "        last_ch = encoder_channels[-1]\n",
    "\n",
    "        # --------------------------\n",
    "        # Simple decoder: upsample to original resolution\n",
    "        # --------------------------\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(last_ch, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1)  # Output: single-channel depth map\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward: returns list of feature maps at different stages\n",
    "        features = self.encoder(x)\n",
    "        x = features[-1]  # Use last-stage feature map for decoder\n",
    "\n",
    "        # Decode to depth map\n",
    "        depth = self.decoder(x)\n",
    "        # Unsample to match input size\n",
    "        depth = F_nn.interpolate(depth, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return depth\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "def train_depth_model(model, train_loader, val_loader, epochs=10, freeze_encoder=True, lr_head=1e-3, lr_finetune=1e-4, model_path=\"depth_model.pth\", best_val_loss=float(\"inf\")):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if freeze_encoder:\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr_head)\n",
    "    else:\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr_finetune)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} {'(Frozen Encoder)' if freeze_encoder else '(Fine-tune)'}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, depths in tqdm(train_loader):\n",
    "            images, depths = images.to(device), depths.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, depths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, depths in val_loader:\n",
    "                images, depths = images.to(device), depths.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, depths)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        rmse = np.sqrt(val_loss)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val RMSE: {rmse:.4f} m\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"✅ Best model saved.\")\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    print(\"Training complete\")\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Device\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# Model\n",
    "# --------------------------\n",
    "model = DepthModel().to(device)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Datasets & Loaders\n",
    "# --------------------------\n",
    "train_folder = \"/kaggle/input/nyu-depth-split-dataset/nyu_split/train\"\n",
    "valid_folder = \"/kaggle/input/nyu-depth-split-dataset/nyu_split/val\"\n",
    "max_depth = 10\n",
    "\n",
    "train_dataset = DepthDataset(train_folder, paired_transform, max_depth)\n",
    "val_dataset = DepthDataset(valid_folder, paired_transform, max_depth)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# --------------------------\n",
    "# Freeze backbone initially\n",
    "# --------------------------\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --------------------------\n",
    "# Training Parameters\n",
    "# --------------------------\n",
    "num_epoch_head = 5       # Train classifier head first\n",
    "num_epoch_finetune = 25  # Fine-tune full model\n",
    "MODEL_PATH = \"/kaggle/working/depth_classifier.pth\"\n",
    "\n",
    "model = DepthModel().to(device)\n",
    "\n",
    "# 1. Train decoder head first\n",
    "best_val_loss = train_depth_model(model, train_loader, val_loader, epochs=num_epoch_head, freeze_encoder=True, \n",
    "                  model_path= MODEL_PATH)\n",
    "\n",
    "# 2. Fine-tune full model\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "train_depth_model(model, train_loader, val_loader, epochs=num_epoch_finetune, freeze_encoder=False, \n",
    "                  model_path=\"depth_model.pth\", best_val_loss = best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bef6e6",
   "metadata": {
    "papermill": {
     "duration": 0.034201,
     "end_time": "2025-09-04T09:20:55.577231",
     "exception": false,
     "start_time": "2025-09-04T09:20:55.543030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1198025,
     "sourceId": 2002504,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8200507,
     "sourceId": 12957283,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4475.524338,
   "end_time": "2025-09-04T09:20:58.489411",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-04T08:06:22.965073",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "4e20e0475dc4435fab51c6d0d4a6ecb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6e545bbdc1a74ae686fb9c72952c3f33",
       "placeholder": "​",
       "style": "IPY_MODEL_d5e78fea17e540d49374d6eab6e61e2f",
       "tabbable": null,
       "tooltip": null,
       "value": " 49.3M/49.3M [00:00&lt;00:00, 137MB/s]"
      }
     },
     "6e545bbdc1a74ae686fb9c72952c3f33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7632d49cc3db4ccd8ca89458ca7fc5c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "793f44f568084e18871a45bdda543e52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "853b2d9fb675410eb864b9071d62865d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "85c5543b07eb43fdbf4b42ed96147479": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a7dc7f7620b2488b8678702ecf8dc3ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e5bb5ac4b60448f4bd58fe4cfc0e6244",
       "placeholder": "​",
       "style": "IPY_MODEL_853b2d9fb675410eb864b9071d62865d",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "d5b48c78306b4acda7d48f50d3574439": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a7dc7f7620b2488b8678702ecf8dc3ca",
        "IPY_MODEL_ed7f486f4fd747afadafbd30b66fecb1",
        "IPY_MODEL_4e20e0475dc4435fab51c6d0d4a6ecb3"
       ],
       "layout": "IPY_MODEL_7632d49cc3db4ccd8ca89458ca7fc5c9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d5e78fea17e540d49374d6eab6e61e2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e5bb5ac4b60448f4bd58fe4cfc0e6244": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed7f486f4fd747afadafbd30b66fecb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_793f44f568084e18871a45bdda543e52",
       "max": 49335454.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_85c5543b07eb43fdbf4b42ed96147479",
       "tabbable": null,
       "tooltip": null,
       "value": 49335454.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
