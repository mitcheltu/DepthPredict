{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "866616e7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-04T07:40:12.368959Z",
     "iopub.status.busy": "2025-09-04T07:40:12.368677Z",
     "iopub.status.idle": "2025-09-04T09:05:54.380831Z",
     "shell.execute_reply": "2025-09-04T09:05:54.378759Z"
    },
    "papermill": {
     "duration": 5142.020377,
     "end_time": "2025-09-04T09:05:54.385047",
     "exception": false,
     "start_time": "2025-09-04T07:40:12.364670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff00952a6d74f5c998d613e6fa713c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:32<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.3079 | Val Loss: 2.1087 | Val RMSE: 1.4521 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 2/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:35<00:00,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7199 | Val Loss: 1.6214 | Val RMSE: 1.2733 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 3/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:35<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3521 | Val Loss: 1.3686 | Val RMSE: 1.1699 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 4/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:35<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1196 | Val Loss: 1.2538 | Val RMSE: 1.1197 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 5/5 (Frozen Encoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:35<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1416 | Val Loss: 1.1631 | Val RMSE: 1.0785 m\n",
      "✅ Best model saved.\n",
      "Training complete.\n",
      "\n",
      "Epoch 1/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:53<00:00,  9.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7817 | Val Loss: 1.4319 | Val RMSE: 1.1966 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 2/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:57<00:00,  9.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9087 | Val Loss: 1.0512 | Val RMSE: 1.0253 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 3/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:56<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6816 | Val Loss: 0.7369 | Val RMSE: 0.8584 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 4/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:48<00:00,  8.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6405 | Val Loss: 0.9133 | Val RMSE: 0.9557 m\n",
      "\n",
      "Epoch 5/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:58<00:00,  9.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5135 | Val Loss: 0.6482 | Val RMSE: 0.8051 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 6/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:48<00:00,  8.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5340 | Val Loss: 0.6896 | Val RMSE: 0.8304 m\n",
      "\n",
      "Epoch 7/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:48<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4729 | Val Loss: 0.6469 | Val RMSE: 0.8043 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 8/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:54<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3966 | Val Loss: 0.6635 | Val RMSE: 0.8146 m\n",
      "\n",
      "Epoch 9/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:54<00:00,  9.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4506 | Val Loss: 0.6330 | Val RMSE: 0.7956 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 10/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:51<00:00,  9.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4114 | Val Loss: 0.5776 | Val RMSE: 0.7600 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 11/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:48<00:00,  8.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3801 | Val Loss: 0.7299 | Val RMSE: 0.8543 m\n",
      "\n",
      "Epoch 12/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:49<00:00,  8.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3925 | Val Loss: 0.6260 | Val RMSE: 0.7912 m\n",
      "\n",
      "Epoch 13/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:53<00:00,  9.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3614 | Val Loss: 0.7369 | Val RMSE: 0.8585 m\n",
      "\n",
      "Epoch 14/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:51<00:00,  9.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3032 | Val Loss: 0.5945 | Val RMSE: 0.7710 m\n",
      "\n",
      "Epoch 15/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:49<00:00,  8.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2716 | Val Loss: 0.5280 | Val RMSE: 0.7267 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 16/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:48<00:00,  8.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2518 | Val Loss: 0.5259 | Val RMSE: 0.7252 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 17/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:46<00:00,  8.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2355 | Val Loss: 0.5498 | Val RMSE: 0.7415 m\n",
      "\n",
      "Epoch 18/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:46<00:00,  8.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2268 | Val Loss: 0.5449 | Val RMSE: 0.7382 m\n",
      "\n",
      "Epoch 19/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:52<00:00,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2162 | Val Loss: 0.5404 | Val RMSE: 0.7351 m\n",
      "\n",
      "Epoch 20/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:55<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2236 | Val Loss: 0.5054 | Val RMSE: 0.7109 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 21/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:56<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2184 | Val Loss: 0.5496 | Val RMSE: 0.7413 m\n",
      "\n",
      "Epoch 22/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:58<00:00,  9.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2144 | Val Loss: 0.5273 | Val RMSE: 0.7262 m\n",
      "\n",
      "Epoch 23/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:56<00:00,  9.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2149 | Val Loss: 0.5047 | Val RMSE: 0.7104 m\n",
      "✅ Best model saved.\n",
      "\n",
      "Epoch 24/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [03:13<00:00, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2090 | Val Loss: 0.5219 | Val RMSE: 0.7224 m\n",
      "\n",
      "Epoch 25/25 (Fine-tune)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [03:21<00:00, 10.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2059 | Val Loss: 0.5309 | Val RMSE: 0.7286 m\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as F_nn\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import timm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def paired_transform(color_img, depth_img, output_size=224, max_depth=10.0, apply_color_jitter=True):\n",
    "    if random.random() > 0.5:\n",
    "        color_img = F.hflip(color_img)\n",
    "        depth_img = F.hflip(depth_img)\n",
    "\n",
    "    angle = random.uniform(-15, 15)\n",
    "    color_img = F.rotate(color_img, angle, interpolation=Image.BILINEAR)\n",
    "    depth_img = F.rotate(depth_img, angle, interpolation=Image.NEAREST)\n",
    "\n",
    "    i, j, h, w = transforms.RandomResizedCrop.get_params(\n",
    "        color_img, scale=(0.8, 1.0), ratio=(1.0, 1.0)\n",
    "    )\n",
    "    color_img = F.resized_crop(color_img, i, j, h, w, size=(output_size, output_size))\n",
    "    depth_img = F.resized_crop(depth_img, i, j, h, w, size=(output_size, output_size))\n",
    "\n",
    "    if apply_color_jitter:\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3)\n",
    "        color_img = color_jitter(color_img)\n",
    "\n",
    "    # RGB → tensor\n",
    "    color_tensor = F.to_tensor(color_img)\n",
    "    color_tensor = F.normalize(color_tensor, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "\n",
    "    # Depth → tensor (meters)\n",
    "    depth_np = np.array(depth_img).astype(np.float32) / 1000.0\n",
    "    depth_tensor = torch.from_numpy(depth_np).unsqueeze(0)\n",
    "    depth_tensor = torch.clamp(depth_tensor, 0, max_depth)\n",
    "\n",
    "    return color_tensor, depth_tensor\n",
    "    \n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, data_dir, paired_transform=None, max_depth=10.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to folder containing 'colors/' and 'depths/' subfolders.\n",
    "            paired_transform (callable, optional): Function to apply same geometric transform to both RGB and depth.\n",
    "            max_depth (float): Maximum depth value to scale depth maps (in meters).\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.paired_transform = paired_transform\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Paths to color and depth folders\n",
    "        self.color_dir = os.path.join(data_dir, \"colors\")\n",
    "        self.depth_dir = os.path.join(data_dir, \"depths\")\n",
    "\n",
    "        # List all RGB color images\n",
    "        self.color_files = sorted([f for f in os.listdir(self.color_dir) if f.endswith(\"_colors.png\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.color_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load RGB image\n",
    "        color_path = os.path.join(self.color_dir, self.color_files[idx])\n",
    "        color_img = Image.open(color_path).convert(\"RGB\")\n",
    "\n",
    "        # Load corresponding depth image\n",
    "        depth_file = self.color_files[idx].replace(\"_colors.png\", \"_depth.png\")\n",
    "        depth_path = os.path.join(self.depth_dir, depth_file)\n",
    "        depth_img = Image.open(depth_path)\n",
    "\n",
    "        # Apply paired transform if provided\n",
    "        if self.paired_transform:\n",
    "            color_tensor, depth_tensor = self.paired_transform(color_img, depth_img)\n",
    "        else:\n",
    "            # Convert RGB to tensor and normalize\n",
    "            color_tensor = T.ToTensor()(color_img)\n",
    "            color_tensor = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                       std=[0.229, 0.224, 0.225])(color_tensor)\n",
    "            # Convert depth to tensor and scale to meters\n",
    "            depth_np = np.array(depth_img).astype(np.float32) / 1000.0  # mm → meters\n",
    "            depth_tensor = torch.from_numpy(depth_np).unsqueeze(0)\n",
    "            depth_tensor = torch.clamp(depth_tensor, 0, self.max_depth)\n",
    "\n",
    "        return color_tensor, depth_tensor\n",
    "\n",
    "\n",
    "class DepthModel(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_b3', pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --------------------------\n",
    "        # Encoder: pretrained CNN\n",
    "        # --------------------------\n",
    "        # Use features_only=True to get intermediate feature maps for decoder\n",
    "        self.encoder = timm.create_model(backbone_name, pretrained=pretrained, features_only=True)\n",
    "        \n",
    "        # Channels of encoder feature maps at each stage\n",
    "        encoder_channels = self.encoder.feature_info.channels()  # e.g., [40, 48, 136, 384]\n",
    "        last_ch = encoder_channels[-1]\n",
    "\n",
    "        # --------------------------\n",
    "        # Simple decoder: upsample to original resolution\n",
    "        # --------------------------\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(last_ch, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1)  # Output: single-channel depth map\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward: returns list of feature maps at different stages\n",
    "        features = self.encoder(x)\n",
    "        x = features[-1]  # Use last-stage feature map for decoder\n",
    "\n",
    "        # Decode to depth map\n",
    "        depth = self.decoder(x)\n",
    "        # Unsample to match input size\n",
    "        depth = F_nn.interpolate(depth, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return depth\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "def train_depth_model(model, train_loader, val_loader, epochs=10, freeze_encoder=True, lr_head=1e-3, lr_finetune=1e-4, model_path=\"depth_model.pth\"):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if freeze_encoder:\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr_head)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} {'(Frozen Encoder)' if freeze_encoder else '(Fine-tune)'}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, depths in tqdm(train_loader):\n",
    "            images, depths = images.to(device), depths.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, depths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, depths in val_loader:\n",
    "                images, depths = images.to(device), depths.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, depths)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        rmse = np.sqrt(val_loss)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val RMSE: {rmse:.4f} m\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"✅ Best model saved.\")\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Device\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# Model\n",
    "# --------------------------\n",
    "model = DepthModel().to(device)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Datasets & Loaders\n",
    "# --------------------------\n",
    "train_folder = \"/kaggle/input/nyu-depth-split-dataset/nyu_split/train\"\n",
    "valid_folder = \"/kaggle/input/nyu-depth-split-dataset/nyu_split/val\"\n",
    "max_depth = 10\n",
    "\n",
    "train_dataset = DepthDataset(train_folder, paired_transform, max_depth)\n",
    "val_dataset = DepthDataset(valid_folder, paired_transform, max_depth)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# --------------------------\n",
    "# Freeze backbone initially\n",
    "# --------------------------\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --------------------------\n",
    "# Training Parameters\n",
    "# --------------------------\n",
    "num_epoch_head = 5       # Train classifier head first\n",
    "num_epoch_finetune = 25  # Fine-tune full model\n",
    "MODEL_PATH = \"/kaggle/working/depth_classifier.pth\"\n",
    "\n",
    "model = DepthModel().to(device)\n",
    "\n",
    "# 1. Train decoder head first\n",
    "train_depth_model(model, train_loader, val_loader, epochs=num_epoch_head, freeze_encoder=True, \n",
    "                  model_path= MODEL_PATH)\n",
    "\n",
    "# 2. Fine-tune full model\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "train_depth_model(model, train_loader, val_loader, epochs=num_epoch_finetune, freeze_encoder=False, \n",
    "                  model_path=\"depth_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1198025,
     "sourceId": 2002504,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8200507,
     "sourceId": 12957283,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5151.02393,
   "end_time": "2025-09-04T09:05:58.410921",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-04T07:40:07.386991",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "21c94e5549d44023aa20dbe017baa1e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5be4c9b63a6c4478bda81c105902fefb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "78f29b6312104382adfeb3f2382963ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d307b148ce284c87916a14271837c2b2",
       "max": 49335454.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c6867cc2e77e480f9015fec2149c6a53",
       "tabbable": null,
       "tooltip": null,
       "value": 49335454.0
      }
     },
     "91a9e21cf1ae40878a607b228f4a6267": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "944a80e4c5384449b34001b274d931f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9b5d2e91e20407f8a86f281a9e6e8c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c354d58f756d4016bf99b617b8919fd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_21c94e5549d44023aa20dbe017baa1e2",
       "placeholder": "​",
       "style": "IPY_MODEL_91a9e21cf1ae40878a607b228f4a6267",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "c6867cc2e77e480f9015fec2149c6a53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d307b148ce284c87916a14271837c2b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7bb938fe68f43f98e9c33c4cdc56a71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_944a80e4c5384449b34001b274d931f4",
       "placeholder": "​",
       "style": "IPY_MODEL_5be4c9b63a6c4478bda81c105902fefb",
       "tabbable": null,
       "tooltip": null,
       "value": " 49.3M/49.3M [00:00&lt;00:00, 165MB/s]"
      }
     },
     "eff00952a6d74f5c998d613e6fa713c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c354d58f756d4016bf99b617b8919fd2",
        "IPY_MODEL_78f29b6312104382adfeb3f2382963ec",
        "IPY_MODEL_e7bb938fe68f43f98e9c33c4cdc56a71"
       ],
       "layout": "IPY_MODEL_a9b5d2e91e20407f8a86f281a9e6e8c5",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
